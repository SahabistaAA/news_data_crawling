icservices:
  # ======================
  # Databases & Storage
  # ======================
  postgres:
    image: postgres:15
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow_11
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.19.1
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - bootstrap.memory_lock=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  kibana:
    image: docker.elastic.co/kibana/kibana:8.19.1
    container_name: kibana
    ports: 
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: "http://elasticsearch:9200"
      SERVER_NAME: kibana
      XPACK_SECURITY_SOLUTION_TELEMETRY_TASKTIMEOUT: 30000
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  neo4j:
    image: neo4j:5
    container_name: neo4j
    environment:
      - NEO4J_AUTH=neo4j/password
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j_data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://localhost:7474 || exit 1"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s

  # ======================
  # Kafka & Zookeeper
  # ======================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.7.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_MAX_CLIENT_CNXNS: 60
      KAFKA_OPTS: "-Dzookeeper.4lw.commands.whitelist=ruok,srvr,stat,mntr,conf"
    ports:
      - "2181:2181"
    volumes:
      - zk_data:/var/lib/zookeeper/data
      - zk_logs:/var/lib/zookeeper/log
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep imok || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.7.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS: 20000

      # Listeners
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

      # Replication & topics
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1

      # Performance
      KAFKA_MESSAGE_MAX_BYTES: 150000000
      KAFKA_REPLICA_FETCH_MAX_BYTES: 150000000
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 150000000
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400

      # Timeouts
      KAFKA_REQUEST_TIMEOUT_MS: 30000
      KAFKA_METADATA_FETCH_TIMEOUT_MS: 60000
      KAFKA_CONNECTIONS_MAX_IDLE_MS: 600000

      # Log retention
      KAFKA_LOG_RETENTION_HOURS: 168         # 7 days
      KAFKA_LOG_SEGMENT_BYTES: 1073741824    # 1GB
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000   # <-- fixed typo here

      # Consumer/Producer
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 3000
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

      # JVM
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      KAFKA_JVM_PERFORMANCE_OPTS: "-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35"
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server kafka:29092 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s  # Give more time for initial startup

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "9000:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      kafka:
        condition: service_healthy

  # ======================
  # Services
  # ======================
  crawler:
    build:
      context: .
      dockerfile: docker/Dockerfile-crawler
    container_name: news-crawler
    environment:
      KAFKA_BROKER: "kafka:9092"
      TOPIC_NAME: "news_topic"

  producer:
    build:
      context: .
      dockerfile: docker/Dockerfile-producer
    container_name: producer
    environment:
      KAFKA_BROKER: "kafka:9092"
      TOPIC_NAME: "news_topic"

  consumer:
    build:
      context: .
      dockerfile: docker/Dockerfile-consumer
    container_name: consumer
    environment:
      KAFKA_BROKER: "kafka:9092"
      TOPIC_NAME: "news_topic"
      POSTGRES_HOST: "postgres"
      ELASTICSEARCH_HOST: "elasticsearch"
      NEO4J_HOST: "neo4j"

  # ======================
  # Airflow (Enhanced from first config)
  # ======================
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile-airflow
    image: news-data-crawling-airflow:latest
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_11@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__WEBSERVER__SECRET_KEY: "dev-secret-key"
      AIRFLOW__CORE__FERNET_KEY: "fHj2b8yU4WJq1eWk9H1N4Q6H7K1V5wWm6b5T6yA9Y8s="
      APP_LOGS_DIR: /opt/airflow/logs/modules
      APP_LOG_LEVEL: INFO
      APP_LOG_MAX_BYTES: "10485760"
      APP_LOG_BACKUP_COUNT: "5"
    entrypoint: ["/bin/bash", "-c"]
    command: >
      "airflow db migrate || airflow db init;
       airflow users create
         --username admin
         --firstname Admin
         --lastname User
         --role Admin
         --email admin@example.com
         --password admin || true"
    volumes:
      - logs:/opt/airflow/logs

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile-airflow
    image: news-data-crawling-airflow:latest
    container_name: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_11@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__WEBSERVER__SECRET_KEY: "dev-secret-key"
      AIRFLOW__CORE__FERNET_KEY: "fHj2b8yU4WJq1eWk9H1N4Q6H7K1V5wWm6b5T6yA9Y8s="
      APP_LOGS_DIR: /opt/airflow/logs/modules
      APP_LOG_LEVEL: INFO
      APP_LOG_MAX_BYTES: "10485760"
      APP_LOG_BACKUP_COUNT: "5"
    command: ["bash", "-lc", "airflow webserver"]
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
    volumes:
      - logs:/opt/airflow/logs
      - ./airflow/dags:/opt/airflow/dags
      - .:/opt/airflow/news_data_crawling

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile-airflow
    image: news-data-crawling-airflow:latest
    container_name: airflow-scheduler
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_11@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__WEBSERVER__SECRET_KEY: "dev-secret-key"
      AIRFLOW__CORE__FERNET_KEY: "fHj2b8yU4WJq1eWk9H1N4Q6H7K1V5wWm6b5T6yA9Y8s="
      APP_LOGS_DIR: /opt/airflow/logs/modules
      APP_LOG_LEVEL: INFO
      APP_LOG_MAX_BYTES: "10485760"
      APP_LOG_BACKUP_COUNT: "5"
    command: ["bash", "-lc", "airflow scheduler"]
    volumes:
      - logs:/opt/airflow/logs
      - ./airflow/dags:/opt/airflow/dags
      - .:/opt/airflow/news_data_crawling

  airflow-worker:
    build:
      context: .
      dockerfile: Dockerfile-airflow
    image: news-data-crawling-airflow:latest
    container_name: airflow-worker
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_11@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: redis://redis:6379/1
      KAFKA_BROKER: "kafka:29092"
      TOPIC_NAME: "news_topic"
      ELASTICSEARCH_HOST: "http://elasticsearch:9200"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - .:/opt/airflow/data_crawling
      - ../data:/opt/airflow/data
      - ../elasticsearch:/opt/airflow/elasticsearch
    command: airflow celery worker
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
    restart: unless-stopped

  flower:
    build:
      context: .
      dockerfile: Dockerfile-airflow
    image: news-data-crawling-airflow:latest
    container_name: flower
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_11@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: redis://redis:6379/1
    ports:
      - "5555:5555"
    command: airflow celery flower
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped

  # ======================
  # Additional Consumer (from first config)
  # ======================
  news-consumer:
    build:
      context: .
      dockerfile: docker/Dockerfile-consumer
    container_name: news-consumer
    environment:
      KAFKA_BROKER: "kafka:29092"
      TOPIC_NAME: "news_topic"
      ELASTICSEARCH_HOST: "elasticsearch"
      CONSUMER_TYPE: "news"
      PYTHONPATH: "/app:/app/elasticsearch"
    volumes:
      - .:/app
      - ./airflow_data:/tmp/airflow_data
    working_dir: /app
    depends_on:
      kafka:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  pg_data:
  redis_data:
  es_data:
  neo4j_data:
  kafka_data:
  zk_data:
  zk_logs:
  logs:
  airflow_logs:
